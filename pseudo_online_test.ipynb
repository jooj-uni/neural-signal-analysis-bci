{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jooj-uni/neural-signal-analysis-bci/blob/master/pseudo_online_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C27fjABSCMjG",
        "outputId": "ac72a7f0-3b69-4b2d-d0db-b5b0d5ca1368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: mne in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2026.1.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: moabb in /usr/local/lib/python3.12/dist-packages (1.4.3)\n",
            "Collecting numpy>=2.0 (from moabb)\n",
            "  Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.16.3)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.11.0)\n",
            "Requirement already satisfied: pandas>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (3.15.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.13.2)\n",
            "Requirement already satisfied: pyriemann>=0.9 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.10)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (6.0.3)\n",
            "Requirement already satisfied: pooch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.8.2)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.32.4)\n",
            "Requirement already satisfied: urllib3>=1.26.15 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (4.67.1)\n",
            "Requirement already satisfied: coverage>=7.0.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (7.13.1)\n",
            "Requirement already satisfied: memory-profiler>=0.61.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.61.0)\n",
            "Requirement already satisfied: edflib-python>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.0.8)\n",
            "Requirement already satisfied: edfio>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.4.11)\n",
            "Requirement already satisfied: pytest>=8.3.5 in /usr/local/lib/python3.12/dist-packages (from moabb) (8.4.2)\n",
            "Requirement already satisfied: mne-bids>=0.16 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.18.0)\n",
            "Collecting scikit-learn>=1.6 (from moabb)\n",
            "  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (2.9.0.post0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory-profiler>=0.61.0->moabb) (5.9.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.2->moabb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.2->moabb) (2025.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.6.0->moabb) (4.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyriemann>=0.9->moabb) (1.5.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (2026.1.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6->moabb) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.2->moabb) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->moabb) (3.0.3)\n",
            "Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.4.1 scikit-learn-1.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "51dfe48497154040a38bc5e0b4973ad1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn<1.4\n",
            "  Using cached scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2.0,>=1.17.3 (from scikit-learn<1.4)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (3.6.0)\n",
            "Using cached scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.1\n",
            "    Uninstalling numpy-2.4.1:\n",
            "      Successfully uninstalled numpy-2.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.8.0\n",
            "    Uninstalling scikit-learn-1.8.0:\n",
            "      Successfully uninstalled scikit-learn-1.8.0\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install mne\n",
        "!pip install moabb\n",
        "!pip install \"scikit-learn<1.4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j5u6USktCRM5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "import seaborn as sns\n",
        "from mne.decoding import CSP\n",
        "from pyriemann.classification import MDM\n",
        "from pyriemann.estimation import Covariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "\n",
        "import moabb\n",
        "from moabb.datasets import BNCI2014_001, Zhou2016, Stieger2021\n",
        "from moabb.evaluations import WithinSessionEvaluation\n",
        "from moabb.paradigms import LeftRightImagery\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7y3CGsryCY-z"
      },
      "outputs": [],
      "source": [
        "# Caminho seguro no Colab\n",
        "mne_path = \"/content/mne_data\"\n",
        "os.environ[\"MNE_DATA\"] = mne_path\n",
        "os.environ[\"MNE_DATASETS_SAMPLE_PATH\"] = mne_path\n",
        "os.environ[\"MOABB_RESULTS\"] = mne_path # Set MOABB results directory using os.environ\n",
        "os.makedirs(mne_path, exist_ok=True)\n",
        "\n",
        "# Forçar MNE a registrar o caminho internamente\n",
        "mne.set_config(\"MNE_DATA\", mne_path, set_env=True)\n",
        "mne.set_config(\"MNE_DATASETS_SAMPLE_PATH\", mne_path, set_env=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I7rylYNkZ7i3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import moabb\n",
        "import mne\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "REST_LABEL = 0\n",
        "REJECT_LABEL = -1\n",
        "\n",
        "class PseudoOnlineWindow():\n",
        "    \"\"\"\n",
        "    Segments data in windows for pseudo-online analysis.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "        raw: mne.Raw object\n",
        "            The continuous data.\n",
        "        events: arra\n",
        "            MNE event array.\n",
        "        interval: list\n",
        "            Dataset parameter defining imagery interval.\n",
        "        task_ids: dict\n",
        "            Defines the tasks and its numeric IDs. It can be used to select a subset of the dataset tasks.\n",
        "        window_size: float\n",
        "            The window size in seconds.\n",
        "        window_step: int\n",
        "            Distance in seconds between the start of two consecutive windows. It can be used to set superposition between windows, when value is lower than window_size.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, raw, events, interval, task_ids, window_size, window_step, chan_list=None):\n",
        "        self.raw = raw\n",
        "        self.events = events\n",
        "        self.interval = interval\n",
        "        self.sfreq = raw.info['sfreq']\n",
        "        self.task_ids = task_ids\n",
        "\n",
        "        self.window_size = int(window_size * self.sfreq)\n",
        "        self.window_step = int(window_step * self.sfreq)\n",
        "        self.chan_list = chan_list\n",
        "\n",
        "        self.t_start = int(interval[0] * self.sfreq)\n",
        "        self.t_end = int(interval[1] * self.sfreq)\n",
        "\n",
        "        self.labels = self.generate_labels()\n",
        "\n",
        "    def generate_labels(self):\n",
        "        \"\"\"\n",
        "        Attributes aa label for each sample. The label vector is initialized with 0 and each data point is attributed to the task label, if it is in imagery period.\n",
        "\n",
        "        Returns:\n",
        "            labels: nd array\n",
        "                Label vector containing labels for each data sample.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples = self.raw.n_times\n",
        "        labels = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "        valid_ids = list(self.task_ids.values())\n",
        "\n",
        "        for ev in self.events:\n",
        "            ev_idx, _, ev_id = ev\n",
        "\n",
        "            if ev_id in valid_ids:\n",
        "                # uses only imagery period for task attribution\n",
        "                start = ev_idx + self.t_start\n",
        "                stop = ev_idx + self.t_end\n",
        "\n",
        "                # ensure array limits\n",
        "                start = max(0, start)\n",
        "                stop = min(n_samples, stop)\n",
        "\n",
        "                labels[start:stop] = ev_id\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def generate_windows(self):\n",
        "        \"\"\"\n",
        "        Generates and labels windows.\n",
        "\n",
        "        Returns:\n",
        "            X: nd array shape=(n_windows, n_channels, n_times)\n",
        "                The windows (data).\n",
        "            y: nd array\n",
        "                Window labels in the same order as X.\n",
        "            times: nd array\n",
        "                Array of tuples. Each tuple is the timestamps (start and end) of each window. Might be useful for plotting.\n",
        "        \"\"\"\n",
        "        X, y, times = [], [], []\n",
        "\n",
        "        data = self.raw.get_data()\n",
        "        n_samples = data.shape[1]\n",
        "\n",
        "        for start_idx in range (0, n_samples - self.window_size, self.window_step):\n",
        "            end_idx = start_idx + self.window_size\n",
        "\n",
        "            if self.chan_list == None:\n",
        "                window_data = data[:, start_idx : end_idx]\n",
        "                window_labels = self.labels[start_idx:end_idx]\n",
        "            else:   #channel selection\n",
        "                window_data = []\n",
        "                for chan in self.chan_list:\n",
        "                    if chan in self.raw.ch_names:\n",
        "                        window_data.append(data[chan, start_idx : end_idx])\n",
        "                    else:\n",
        "                        raise ValueError(f\"Channel {chan} is not in {self.raw.ch_names}\")\n",
        "                window_labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "            count = np.bincount(window_labels)\n",
        "            major = np.argmax(count)\n",
        "\n",
        "            prop_major = count[major] / len(window_labels)\n",
        "\n",
        "            # class draw proportion\n",
        "            n_classes = len(np.unique(window_labels))\n",
        "            draw_prop = 1 / n_classes\n",
        "\n",
        "            # in case of draw, the posterior label wins\n",
        "            if prop_major != draw_prop:\n",
        "                y.append(major)\n",
        "            else:\n",
        "                y.append(window_labels[-1])\n",
        "\n",
        "            X.append(window_data)\n",
        "            times.append(((start_idx / self.sfreq), (end_idx / self.sfreq)))\n",
        "\n",
        "        return np.array(X), np.array(y), np.array(times)\n",
        "\n",
        "\n",
        "class IdleBaseline(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Applies baseline correction. It uses a fixed baseline. This transformer has to be applied to windowed data.\n",
        "\n",
        "    Parameters:\n",
        "        rest_label: int\n",
        "            Label representing idle state.\n",
        "\n",
        "    Returns:\n",
        "        X: nd array shape=(n_windows, n_channels, n_times)\n",
        "            Baseline corrected windowed data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rest_label=REST_LABEL):\n",
        "        self.rest_label = rest_label\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if y is None:\n",
        "            raise ValueError(\"Missing labels array\")\n",
        "\n",
        "        idle_windows = (y == self.rest_label)\n",
        "\n",
        "        if not np.any(idle_windows):\n",
        "            raise ValueError(\"There are no rest windows\")\n",
        "\n",
        "        self.baseline_ = X[idle_windows].mean(axis=0)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.subtract(X, self.baseline_)\n",
        "\n",
        "\n",
        "class PSD(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, sfreq, fmin=0, fmax=None):\n",
        "        self.sfreq = sfreq\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "\n",
        "    def fit (self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        fmax = self.fmax if self.fmax is not None else self.sfreq / 2\n",
        "\n",
        "        psds, _ = mne.time_frequency.psd_array_welch(X,\n",
        "                                                     sfreq=self.sfreq,\n",
        "                                                     fmin=self.fmin,\n",
        "                                                     fmax=self.fmax,\n",
        "                                                     average='mean')\n",
        "\n",
        "        return psds\n",
        "\n",
        "class IdleDetection():\n",
        "    \"\"\"\n",
        "    Classifier for idle state detection.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, threshold=0.6):\n",
        "        self.threshold = threshold\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_idle = (y != REST_LABEL).astype(int)\n",
        "        self.model.fit(X, y_idle)\n",
        "        return self\n",
        "\n",
        "    def is_idle(self, window):\n",
        "        p_idle = self.model.predict_proba(window)[0, 0]\n",
        "\n",
        "        if p_idle < self.threshold:\n",
        "            return 0, p_idle\n",
        "        else:\n",
        "            return 1, p_idle\n",
        "\n",
        "\n",
        "class PseudoOnlineEvaluation():\n",
        "    \"\"\"\n",
        "    Evaluates windowed data in pseudo-online manner (simulating real time asynchronous BCI). It can be done within-session, evaluating only one session data, or inter-session, evaluating performance across sessions, without\n",
        "    breaking signal causality.\n",
        "\n",
        "    Feature pipeline should only be used to apply transformers common to all classification pipelines (such as IdleBaseline and PSD). The result in feature_pipeline has to preserve (n_samples, n_x, n_y) shape. If no feature_pipeline\n",
        "    is present, idle_detector will user reshaped data, and all classification pipelines will have to use a FunctionTransformer to reshape data, if only the classifier is used.\n",
        "\n",
        "    Parameters:\n",
        "        dataset: MOABB dataset\n",
        "            Dataset to be used. It has to be a MOABB dataset object.\n",
        "        pipelines: dict\n",
        "            Sklearn pipelines dictionary, as used in MOABB.\n",
        "        method: string\n",
        "            Either 'within-session' or 'inter-session'.\n",
        "                within-session: trains models on first k windows, testing on the remaining ones.\n",
        "                inter-session: trains models on first k sessions, testing on the remaining ones; doesn't violate data causality.\n",
        "        wsize: float\n",
        "            Window size.\n",
        "        wstep: int\n",
        "            Distance between consecutive windows (start to start).\n",
        "        subjects: list\n",
        "            List of subjects to process.\n",
        "        ratio: float\n",
        "            Proportion of data to be used in training.\n",
        "        threshold: float\n",
        "            Confidence threshold for classification.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, class_pipelines, method, wsize, wstep, subjects, feature_pipeline=None, ratio=0.7, threshold=0.6, no_run=False):\n",
        "        self.dataset = dataset\n",
        "        self.feature_pipeline = feature_pipeline\n",
        "        self.class_pipelines = class_pipelines\n",
        "        self.ratio = ratio\n",
        "        self.method = method\n",
        "        self.wsize = wsize\n",
        "        self.wstep = wstep\n",
        "        self.subjects = subjects\n",
        "        self.task_threshold = threshold\n",
        "        self.no_run = no_run\n",
        "\n",
        "        self.results_ = []\n",
        "        self.model_results_ = []\n",
        "\n",
        "    def raw_concat(self, raw_list):\n",
        "        \"\"\"\n",
        "        Auxiliary function for raw data concat.\n",
        "        \"\"\"\n",
        "        if len(raw_list) == 0:\n",
        "            raise ValueError(\"Raw list is empty.\")\n",
        "        elif len(raw_list) == 1:    #aqui, o raw de uma sessao pode ser constituido de 1 ou mais runs, por isso essa verificação\n",
        "            if type(raw_list[0]) != list:\n",
        "                return raw_list[0]\n",
        "            else:\n",
        "                return mne.concatenate_raws(raw_list[0])\n",
        "        else:\n",
        "            return mne.concatenate_raws(raw_list)\n",
        "\n",
        "    def window_process(self, subject, sess, X_test, y_test, times_test):\n",
        "        # window processing loop\n",
        "        for window in range(len(X_test)):\n",
        "            window_start = times_test[window][0]\n",
        "            window_end = times_test[window][1]\n",
        "\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            if (self.feature_pipeline != None):\n",
        "                feature_window = self.feature_pipeline.transform([X_test[window]])\n",
        "            else:\n",
        "                feature_window = np.array([X_test[window]])\n",
        "                idle_window = np.reshape(feature_window, (feature_window.shape[0], -1))\n",
        "\n",
        "            t_end = time.perf_counter()\n",
        "\n",
        "            t_transform = t_end - t_start\n",
        "\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            idle_window, idle_proba = self.idle_detector.is_idle(idle_window)\n",
        "\n",
        "            t_end = time.perf_counter()\n",
        "            t_idle_detect = t_end - t_start\n",
        "\n",
        "            task_proba = None\n",
        "\n",
        "            if not idle_window:\n",
        "                for name, pipe in self.class_pipelines.items():\n",
        "                    t_start = time.perf_counter()\n",
        "\n",
        "                    probs = pipe.predict_proba(feature_window)[0]\n",
        "\n",
        "                    task_proba = np.max(probs)\n",
        "\n",
        "                    if task_proba < self.task_threshold:\n",
        "                        y_pred = REJECT_LABEL\n",
        "                    else:\n",
        "                        y_pred = probs.argmax()\n",
        "\n",
        "                    t_end = time.perf_counter()\n",
        "\n",
        "                    t_task_predict = t_end - t_start\n",
        "\n",
        "                    correct = (y_pred == y_test[window])\n",
        "\n",
        "                    res = {\n",
        "                        \"dataset\": self.dataset,\n",
        "                        \"subject\": subject,\n",
        "                        \"session\": sess,\n",
        "                        \"method\": self.method,\n",
        "                        \"pipeline\": name,\n",
        "                        \"window\": window,\n",
        "                        \"window_start\": window_start,\n",
        "                        \"window_end\": window_end,\n",
        "                        \"is_idle\": idle_window,\n",
        "                        \"t_transform\": t_transform,\n",
        "                        \"t_idle_detect\": t_idle_detect,\n",
        "                        \"t_task_predict\": t_task_predict,\n",
        "                        \"t_predict\": (t_transform + t_idle_detect + t_task_predict),\n",
        "                        \"y_pred\": y_pred,\n",
        "                        \"idle_proba\": idle_proba,\n",
        "                        \"task_proba\": task_proba,\n",
        "                        \"y_true\": y_test[window],\n",
        "                        \"correct\": correct\n",
        "                    }\n",
        "\n",
        "                    self.results_.append(res)\n",
        "            else:\n",
        "                y_pred = REST_LABEL\n",
        "                t_task_predict = 0\n",
        "                correct = (y_pred == y_test[window])\n",
        "\n",
        "                res = {\n",
        "                    \"dataset\": self.dataset,\n",
        "                    \"subject\": subject,\n",
        "                    \"session\": sess,\n",
        "                    \"method\": self.method,\n",
        "                    \"pipeline\": \"idle\",\n",
        "                    \"window\": window,\n",
        "                    \"window_start\": window_start,\n",
        "                    \"window_end\": window_end,\n",
        "                    \"is_idle\": idle_window,\n",
        "                    \"t_transform\": t_transform,\n",
        "                    \"t_idle_detect\": t_idle_detect,\n",
        "                    \"t_task_predict\": t_task_predict,\n",
        "                    \"t_predict\": (t_transform + t_idle_detect + t_task_predict),\n",
        "                    \"y_pred\": y_pred,\n",
        "                    \"idle_proba\": idle_proba,\n",
        "                    \"task_proba\": task_proba,\n",
        "                    \"y_true\": y_test[window],\n",
        "                    \"correct\": correct\n",
        "                }\n",
        "\n",
        "                self.results_.append(res)\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Main function for processing data.\n",
        "        \"\"\"\n",
        "\n",
        "        self.idle_detector = IdleDetection(LogisticRegression())\n",
        "\n",
        "        for subject in self.subjects:\n",
        "            if subject not in self.dataset.subject_list:\n",
        "                raise ValueError(f\"Invalid subject index: {subject}\")\n",
        "            else:\n",
        "                print(f\"Processing subject {subject}...\")\n",
        "\n",
        "                raws_dict = {}\n",
        "                raws_test = {}\n",
        "                pre = self.dataset.get_data(subjects=[subject])\n",
        "\n",
        "                session_keys = []   # stores session ids (not always int)\n",
        "\n",
        "                if self.method == 'within-session':\n",
        "\n",
        "                    # raw extraction from moabb dataset\n",
        "                    for _, runs in pre.items():\n",
        "                        for sess, dicts in runs.items():\n",
        "                            session_keys.append(sess)\n",
        "                            raws_dict[sess] = []\n",
        "                            for _, data in dicts.items():\n",
        "                                raws_dict[sess].append(data)\n",
        "\n",
        "                    for sess in session_keys:\n",
        "                        print(f\"Processing session {sess} subject {subject}...\")\n",
        "                        raw = self.raw_concat(raws_dict[sess])\n",
        "                        events, event_ids = mne.events_from_annotations(raw)\n",
        "\n",
        "                        wgen = PseudoOnlineWindow(raw=raw,\n",
        "                                                events=events,\n",
        "                                                interval=self.dataset.interval,\n",
        "                                                task_ids=event_ids,\n",
        "                                                window_size=self.wsize,\n",
        "                                                window_step=self.wstep\n",
        "                                                )\n",
        "\n",
        "                        X, y, times = wgen.generate_windows()\n",
        "\n",
        "                        idx_split = int(len(X) * self.ratio)\n",
        "\n",
        "                        times_test = times[idx_split:]\n",
        "\n",
        "                        X_train, y_train = X[:idx_split], y[:idx_split]\n",
        "                        X_test, y_test = X[idx_split:], y[idx_split:]\n",
        "\n",
        "                        if (self.no_run):\n",
        "                            return X_train, y_train, X_test, y_test\n",
        "\n",
        "                        # feature pipeline training\n",
        "                        if (self.feature_pipeline != None):\n",
        "                          t_start = time.perf_counter()\n",
        "                          self.feature_pipeline.fit(X_train, y_train)\n",
        "                          t_end = time.perf_counter()\n",
        "\n",
        "                          t_feature_train = t_end - t_start\n",
        "\n",
        "                          res = {\n",
        "                              \"pipeline\": \"feature\",\n",
        "                              \"method\": self.method,\n",
        "                              \"t_train\": t_feature_train\n",
        "\n",
        "                          }\n",
        "\n",
        "                          self.model_results_.append(res)\n",
        "\n",
        "                          X_train = self.feature_pipeline.transform(X_train)\n",
        "\n",
        "                        # idle detector training\n",
        "                        if (self.feature_pipeline == None):\n",
        "                            idle_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "                        t_start = time.perf_counter()\n",
        "                        self.idle_detector.fit(idle_train, y_train)\n",
        "                        t_end = time.perf_counter()\n",
        "\n",
        "                        t_idle_train = t_end - t_start\n",
        "\n",
        "                        res = {\n",
        "                            \"pipeline\": \"idle\",\n",
        "                            \"method\": self.method,\n",
        "                            \"t_train\": t_idle_train\n",
        "\n",
        "                        }\n",
        "\n",
        "                        self.model_results_.append(res)\n",
        "\n",
        "                        # mask for task windows\n",
        "                        mask = y_train != REST_LABEL\n",
        "                        X_task = X_train[mask]\n",
        "                        y_task = y_train[mask]\n",
        "\n",
        "                        # task classifier training\n",
        "                        for name, model in self.class_pipelines.items():\n",
        "                            print(\"Fitting task classifier...\")\n",
        "                            t_start = time.perf_counter()\n",
        "                            model.fit(X_task, y_task)\n",
        "                            t_end = time.perf_counter()\n",
        "\n",
        "                            t_train = t_end - t_start\n",
        "                            print(\"Done fitting!\")\n",
        "\n",
        "                            res = {\n",
        "                                \"pipeline\": name,\n",
        "                                \"method\": self.method,\n",
        "                                \"t_train\": t_train\n",
        "\n",
        "                            }\n",
        "\n",
        "                            self.model_results_.append(res)\n",
        "\n",
        "                        self.window_process(subject, sess, X_test, y_test, times_test)\n",
        "\n",
        "\n",
        "                elif self.method == 'inter-session':\n",
        "                    if self.dataset.n_sessions > 1:\n",
        "                        session_split = int(self.ratio * self.dataset.n_sessions)\n",
        "                        raws_list = []\n",
        "                        raws_train = []\n",
        "\n",
        "                        print(f\"Splitting index is {session_split}, dataset has {self.dataset.n_sessions} sessions per subject.\")\n",
        "\n",
        "                        for _, runs in pre.items():\n",
        "                            for sess, dicts in runs.items():\n",
        "                                session_keys.append(sess)\n",
        "                                raws_test[sess] = []\n",
        "                                raws_dict[sess] = []\n",
        "                                for _, data in dicts.items():\n",
        "                                    raws_dict[sess].append(data)\n",
        "\n",
        "                        #essa verificação é porque eu acahva que o int() arredondava pra cima o valor... de todo jeito, nao faz mal deixar isso aqui\n",
        "                        if session_split == self.dataset.n_sessions:\n",
        "                            train_sessions = session_keys[:(session_split - 1)]\n",
        "                            test_sessions = session_keys[(session_split - 1):]\n",
        "                            for sess, data in raws_dict.items():\n",
        "                                if (session_keys.index(sess) + 1) < session_split:\n",
        "                                    raws_list.append(data)\n",
        "                                else:\n",
        "                                    raws_test[sess].append(data)\n",
        "                        else:\n",
        "                            train_sessions = session_keys[:session_split]\n",
        "                            test_sessions = session_keys[session_split:]\n",
        "                            for sess, data in raws_dict.items():\n",
        "                                if (session_keys.index(sess) + 1) <= session_split:\n",
        "                                    raws_list.append(data)\n",
        "                                else:\n",
        "                                    raws_test[sess].append(data)\n",
        "\n",
        "\n",
        "                        raws_train = self.raw_concat(raws_list)\n",
        "\n",
        "                        events, event_ids = mne.events_from_annotations(raws_train)\n",
        "\n",
        "                        wgen_train = PseudoOnlineWindow(raw=raws_train,\n",
        "                                                            events=events,\n",
        "                                                            interval=self.dataset.interval,\n",
        "                                                            task_ids=event_ids,\n",
        "                                                            window_size=self.wsize,\n",
        "                                                            window_step=self.wstep\n",
        "                                                            )\n",
        "\n",
        "                        X_train, y_train, times_train = wgen_train.generate_windows()\n",
        "\n",
        "                        if (self.no_run):\n",
        "                            return X_train, y_train, X_test, y_test\n",
        "\n",
        "                        # feature pipeline training\n",
        "                        t_start = time.perf_counter()\n",
        "                        self.feature_pipeline.fit(X_train, y_train)\n",
        "                        t_end = time.perf_counter()\n",
        "\n",
        "                        t_feature_train = t_end - t_start\n",
        "\n",
        "                        res = {\n",
        "                            \"pipeline\": \"feature\",\n",
        "                            \"method\": self.method,\n",
        "                            \"t_train\": t_feature_train\n",
        "\n",
        "                        }\n",
        "\n",
        "                        self.model_results_.append(res)\n",
        "\n",
        "                        X_train = self.feature_pipeline.transform(X_train)\n",
        "\n",
        "                        # idle detector training\n",
        "                        t_start = time.perf_counter()\n",
        "                        self.idle_detector.fit(X_train, y_train)\n",
        "                        t_end = time.perf_counter()\n",
        "\n",
        "                        t_idle_train = t_end - t_start\n",
        "\n",
        "                        res = {\n",
        "                            \"pipeline\": \"idle\",\n",
        "                            \"method\": self.method,\n",
        "                            \"t_train\": t_idle_train\n",
        "\n",
        "                        }\n",
        "\n",
        "                        self.model_results_.append(res)\n",
        "\n",
        "                        # mask for task windows\n",
        "                        mask = y_train != REST_LABEL\n",
        "                        X_task = X_train[mask]\n",
        "                        y_task = y_train[mask]\n",
        "\n",
        "                        # task classifier training\n",
        "                        for name, model in self.class_pipelines.items():\n",
        "                            print(\"Fitting task classifier...\")\n",
        "                            t_start = time.perf_counter()\n",
        "                            model.fit(X_task, y_task)\n",
        "                            t_end = time.perf_counter()\n",
        "\n",
        "                            t_train = t_end - t_start\n",
        "                            print(\"Done fitting!\")\n",
        "\n",
        "                            res = {\n",
        "                                \"pipeline\": name,\n",
        "                                \"method\": self.method,\n",
        "                                \"t_train\": t_train\n",
        "\n",
        "                            }\n",
        "\n",
        "                            self.model_results_.append(res)\n",
        "\n",
        "                        for sess in test_sessions:\n",
        "                            print(f\"Testing in session {sess}...\")\n",
        "\n",
        "                            raws = self.raw_concat(raws_test[sess])\n",
        "\n",
        "                            events, event_ids = mne.events_from_annotations(raws)\n",
        "\n",
        "                            wgen_test = PseudoOnlineWindow(raw=raws,\n",
        "                                                                events=events,\n",
        "                                                                interval=self.dataset.interval,\n",
        "                                                                task_ids=event_ids,\n",
        "                                                                window_size=self.wsize,\n",
        "                                                                window_step=self.wstep\n",
        "                                                                )\n",
        "\n",
        "                            X_test, y_test, times_test = wgen_test.generate_windows()\n",
        "\n",
        "                            self.window_process(subject, sess, X_test, y_test, times_test)\n",
        "                    else:\n",
        "                        raise ValueError(\"There are not enough sessions for evaluation.\")\n",
        "\n",
        "            if len(self.results_):\n",
        "                results_ = pd.DataFrame(self.results_)\n",
        "                results_.to_csv(f\"results-S{subject}.csv\", index=False)\n",
        "                self.results_ = []\n",
        "            if len(self.model_results_):\n",
        "                model_results_ = pd.DataFrame(self.model_results_)\n",
        "                model_results_.to_csv(f\"model-results-S{subject}.csv\", index=False)\n",
        "                self.model_results_ = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                   # mcc_acc_sess = matthews_corrcoef(y_test[:window+1], predictions_sess[:window+1])    #score acumulado dentro da sessão\n",
        "                                   # mcc_acc = matthews_corrcoef(y_all, predictions)   #score acumulado entre sessões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPmx2FnwDJdG",
        "outputId": "71464306-fefb-4bd3-cdcd-4c4304292ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 1...\n",
            "Processing session 0train subject 1...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Fitting task classifier...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.14 (2.2e-16 eps * 26 dim * 2.4e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task classifier...\n",
            "Done fitting!\n",
            "Processing session 1test subject 1...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Fitting task classifier...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.14 (2.2e-16 eps * 26 dim * 2.4e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task classifier...\n",
            "Done fitting!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat' to file '/content/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02T.mat'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'bnci-horizon-2020.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "100%|█████████████████████████████████████| 43.1M/43.1M [00:00<00:00, 43.1GB/s]\n",
            "SHA256 hash of downloaded file: 5ddd5cb520b1692c3ba1363f48d98f58f0e46f3699ee50d749947950fc39db27\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat' to file '/content/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02E.mat'.\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'bnci-horizon-2020.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "100%|█████████████████████████████████████| 44.2M/44.2M [00:00<00:00, 35.0GB/s]\n",
            "SHA256 hash of downloaded file: d63c454005d3a9b41d8440629482e855afc823339bdd0b5721842a7ee9cc7b12\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing session 0train subject 2...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Fitting task classifier...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.14 (2.2e-16 eps * 26 dim * 2.4e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task classifier...\n",
            "Done fitting!\n",
            "Processing session 1test subject 2...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Fitting task classifier...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.14 (2.2e-16 eps * 26 dim * 2.4e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task classifier...\n",
            "Done fitting!\n"
          ]
        }
      ],
      "source": [
        "dataset = BNCI2014_001()\n",
        "\n",
        "pipelines = {}\n",
        "pipelines[\"csp+lda\"] = make_pipeline(CSP(n_components=4), LDA())\n",
        "pipelines[\"tgsp+svc\"] = make_pipeline(Covariances(estimator='lwf'), TangentSpace(), SVC(kernel=\"linear\", probability=True))\n",
        "\n",
        "eval = PseudoOnlineEvaluation(dataset=dataset, class_pipelines=pipelines, method='within-session', wsize=0.4, wstep=0.4, ratio=0.7, subjects=[1, 2])\n",
        "eval.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50BIEspG9cu0"
      },
      "outputs": [],
      "source": [
        "def plot_inter_scores (df, subject, sess):\n",
        "  df_sub = df[(df[\"subject\"] == subject) & (df[\"session\"] == sess)].copy()\n",
        "\n",
        "  df_sub = df_sub.sort_values(\"window\")\n",
        "\n",
        "  plt.figure(figsize=(25, 10))\n",
        "  plt.plot(df_sub[\"window\"], df_sub[\"mcc_acc\"])\n",
        "  plt.xlabel(\"Window\")\n",
        "  plt.ylabel(\"Score acumulado\")\n",
        "  plt.title(f\"Sujeito {subject} - Sessão {sess}: Score Acumulado\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uRq77c8dOft"
      },
      "outputs": [],
      "source": [
        "plot_inter_scores(eval.results_, 1, '1test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZbExL//tpyRBj3s7BoWGf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}