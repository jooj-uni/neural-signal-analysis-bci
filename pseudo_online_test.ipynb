{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jooj-uni/neural-signal-analysis-bci/blob/master/pseudo_online_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C27fjABSCMjG",
        "outputId": "2883d8d6-039f-4b4d-e77e-aa35ed0c9739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: mne in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2026.1.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: moabb in /usr/local/lib/python3.12/dist-packages (1.4.3)\n",
            "Collecting numpy>=2.0 (from moabb)\n",
            "  Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.16.3)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.11.0)\n",
            "Requirement already satisfied: pandas>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (3.15.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.13.2)\n",
            "Requirement already satisfied: pyriemann>=0.9 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.10)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (6.0.3)\n",
            "Requirement already satisfied: pooch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.8.2)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.32.4)\n",
            "Requirement already satisfied: urllib3>=1.26.15 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (4.67.1)\n",
            "Requirement already satisfied: coverage>=7.0.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (7.13.1)\n",
            "Requirement already satisfied: memory-profiler>=0.61.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.61.0)\n",
            "Requirement already satisfied: edflib-python>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.0.8)\n",
            "Requirement already satisfied: edfio>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.4.12)\n",
            "Requirement already satisfied: pytest>=8.3.5 in /usr/local/lib/python3.12/dist-packages (from moabb) (8.4.2)\n",
            "Requirement already satisfied: mne-bids>=0.16 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.18.0)\n",
            "Collecting scikit-learn>=1.6 (from moabb)\n",
            "  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (2.9.0.post0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory-profiler>=0.61.0->moabb) (5.9.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.2->moabb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.2->moabb) (2025.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.6.0->moabb) (4.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyriemann>=0.9->moabb) (1.5.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (2026.1.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6->moabb) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.2->moabb) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->moabb) (3.0.3)\n",
            "Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.4.1 scikit-learn-1.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "7d04422a732547feab69a3076e43e414"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn<1.4\n",
            "  Using cached scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2.0,>=1.17.3 (from scikit-learn<1.4)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (3.6.0)\n",
            "Using cached scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.1\n",
            "    Uninstalling numpy-2.4.1:\n",
            "      Successfully uninstalled numpy-2.4.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.8.0\n",
            "    Uninstalling scikit-learn-1.8.0:\n",
            "      Successfully uninstalled scikit-learn-1.8.0\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install mne\n",
        "!pip install moabb\n",
        "!pip install \"scikit-learn<1.4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j5u6USktCRM5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "import seaborn as sns\n",
        "from mne.decoding import CSP\n",
        "from pyriemann.classification import MDM\n",
        "from pyriemann.estimation import Covariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "\n",
        "import moabb\n",
        "from moabb.datasets import BNCI2014_001, Zhou2016, Stieger2021\n",
        "from moabb.evaluations import WithinSessionEvaluation\n",
        "from moabb.paradigms import LeftRightImagery\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7y3CGsryCY-z"
      },
      "outputs": [],
      "source": [
        "# Caminho seguro no Colab\n",
        "mne_path = \"/content/mne_data\"\n",
        "os.environ[\"MNE_DATA\"] = mne_path\n",
        "os.environ[\"MNE_DATASETS_SAMPLE_PATH\"] = mne_path\n",
        "os.environ[\"MOABB_RESULTS\"] = mne_path # Set MOABB results directory using os.environ\n",
        "os.makedirs(mne_path, exist_ok=True)\n",
        "\n",
        "# Forçar MNE a registrar o caminho internamente\n",
        "mne.set_config(\"MNE_DATA\", mne_path, set_env=True)\n",
        "mne.set_config(\"MNE_DATASETS_SAMPLE_PATH\", mne_path, set_env=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I7rylYNkZ7i3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import moabb\n",
        "import mne\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "REST_LABEL = 0\n",
        "REJECT_LABEL = -1\n",
        "\n",
        "class PseudoOnlineWindow():\n",
        "    \"\"\"\n",
        "    Segments data in windows for pseudo-online analysis.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "        raw: mne.Raw object\n",
        "            The continuous data.\n",
        "        events: arra\n",
        "            MNE event array.\n",
        "        interval: list\n",
        "            Dataset parameter defining imagery interval.\n",
        "        task_ids: dict\n",
        "            Defines the tasks and its numeric IDs. It can be used to select a subset of the dataset tasks.\n",
        "        window_size: float\n",
        "            The window size in seconds.\n",
        "        window_step: int\n",
        "            Distance in seconds between the start of two consecutive windows. It can be used to set superposition between windows, when value is lower than window_size.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, raw, events, interval, task_ids, window_size, window_step, chan_list=None):\n",
        "        self.raw = raw\n",
        "        self.events = events\n",
        "        self.interval = interval\n",
        "        self.sfreq = raw.info['sfreq']\n",
        "        self.task_ids = task_ids\n",
        "\n",
        "        self.window_size = int(window_size * self.sfreq)\n",
        "        self.window_step = int(window_step * self.sfreq)\n",
        "        self.chan_list = chan_list\n",
        "\n",
        "        self.t_start = int(interval[0] * self.sfreq)\n",
        "        self.t_end = int(interval[1] * self.sfreq)\n",
        "\n",
        "        self.labels = self.generate_labels()\n",
        "\n",
        "    def generate_labels(self):\n",
        "        \"\"\"\n",
        "        Attributes aa label for each sample. The label vector is initialized with 0 and each data point is attributed to the task label, if it is in imagery period.\n",
        "\n",
        "        Returns:\n",
        "            labels: nd array\n",
        "                Label vector containing labels for each data sample.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples = self.raw.n_times\n",
        "        labels = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "        valid_ids = list(self.task_ids.values())\n",
        "\n",
        "        for ev in self.events:\n",
        "            ev_idx, _, ev_id = ev\n",
        "\n",
        "            if ev_id in valid_ids:\n",
        "                # uses only imagery period for task attribution\n",
        "                start = ev_idx + self.t_start\n",
        "                stop = ev_idx + self.t_end\n",
        "\n",
        "                # ensure array limits\n",
        "                start = max(0, start)\n",
        "                stop = min(n_samples, stop)\n",
        "\n",
        "                labels[start:stop] = ev_id\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def generate_windows(self):\n",
        "        \"\"\"\n",
        "        Generates and labels windows.\n",
        "\n",
        "        Returns:\n",
        "            X: nd array shape=(n_windows, n_channels, n_times)\n",
        "                The windows (data).\n",
        "            y: nd array\n",
        "                Window labels in the same order as X.\n",
        "            times: nd array\n",
        "                Array of tuples. Each tuple is the timestamps (start and end) of each window. Might be useful for plotting.\n",
        "        \"\"\"\n",
        "        X, y, times = [], [], []\n",
        "\n",
        "        data = self.raw.get_data()\n",
        "        n_samples = data.shape[1]\n",
        "\n",
        "        for start_idx in range (0, n_samples - self.window_size, self.window_step):\n",
        "            end_idx = start_idx + self.window_size\n",
        "\n",
        "            if self.chan_list == None:\n",
        "                window_data = data[:, start_idx : end_idx]\n",
        "                window_labels = self.labels[start_idx:end_idx]\n",
        "            else:   #channel selection\n",
        "                window_data = []\n",
        "                for chan in self.chan_list:\n",
        "                    if chan in self.raw.ch_names:\n",
        "                        window_data.append(data[chan, start_idx : end_idx])\n",
        "                    else:\n",
        "                        raise ValueError(f\"Channel {chan} is not in {self.raw.ch_names}\")\n",
        "                window_labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "            count = np.bincount(window_labels)\n",
        "            major = np.argmax(count)\n",
        "\n",
        "            prop_major = count[major] / len(window_labels)\n",
        "\n",
        "            # class draw proportion\n",
        "            n_classes = len(np.unique(window_labels))\n",
        "            draw_prop = 1 / n_classes\n",
        "\n",
        "            # in case of draw, the posterior label wins\n",
        "            if prop_major != draw_prop:\n",
        "                y.append(major)\n",
        "            else:\n",
        "                y.append(window_labels[-1])\n",
        "\n",
        "            X.append(window_data)\n",
        "            times.append(((start_idx / self.sfreq), (end_idx / self.sfreq)))\n",
        "\n",
        "        return np.array(X), np.array(y), np.array(times)\n",
        "\n",
        "\n",
        "class IdleBaseline(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Applies baseline correction. It uses a fixed baseline. This transformer has to be applied to windowed data.\n",
        "\n",
        "    Parameters:\n",
        "        rest_label: int\n",
        "            Label representing idle state.\n",
        "\n",
        "    Returns:\n",
        "        X: nd array shape=(n_windows, n_channels, n_times)\n",
        "            Baseline corrected windowed data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rest_label=REST_LABEL):\n",
        "        self.rest_label = rest_label\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if y is None:\n",
        "            raise ValueError(\"Missing labels array\")\n",
        "\n",
        "        idle_windows = (y == self.rest_label)\n",
        "\n",
        "        if not np.any(idle_windows):\n",
        "            raise ValueError(\"There are no rest windows\")\n",
        "\n",
        "        self.baseline_ = X[idle_windows].mean(axis=0)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.subtract(X, self.baseline_)\n",
        "\n",
        "\n",
        "class ArrayFilter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    If output is used directly on classifier, function transformer to reshape data needs to be used.\n",
        "    \"\"\"\n",
        "    def __init__(self, sfreq, lfreq, hfreq):\n",
        "        self.sfreq = sfreq\n",
        "        self.lfreq = lfreq\n",
        "        self.hfreq = hfreq\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        b, a = butter(\n",
        "        N=4,\n",
        "        Wn=[self.lfreq, self.hfreq],\n",
        "        btype='bandpass',\n",
        "        fs=self.sfreq\n",
        "        )\n",
        "\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_filt = lfilter(self.b_, self.a_, X, axis=-1)\n",
        "\n",
        "        return np.array(X_filt)\n",
        "\n",
        "class PSD(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    If output is used directly on classifier, function transformer to reshape data needs to be used.\n",
        "    \"\"\"\n",
        "    def __init__(self, sfreq, fmin=0, fmax=None):\n",
        "        self.sfreq = sfreq\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "\n",
        "    def fit (self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        fmax = self.fmax if self.fmax is not None else self.sfreq / 2\n",
        "\n",
        "        n_seg = int(len(X[0, 0, :])/2)\n",
        "        if n_seg > 256:\n",
        "            n_fft = n_seg\n",
        "        else:\n",
        "            n_fft = 256\n",
        "\n",
        "        psds, _ = mne.time_frequency.psd_array_welch(X,\n",
        "                                                     sfreq=self.sfreq,\n",
        "                                                     fmin=self.fmin,\n",
        "                                                     fmax=self.fmax,\n",
        "                                                     n_per_seg=n_seg,\n",
        "                                                     n_fft=n_fft,\n",
        "                                                     average='mean')\n",
        "\n",
        "        psds = np.array(psds)\n",
        "\n",
        "        return psds\n",
        "\n",
        "\"\"\"\n",
        "deprecated\n",
        "\n",
        "class IdleDetection():\n",
        "    #Classifier for idle state detection.\n",
        "    def __init__(self, model, threshold=0.6):\n",
        "        self.threshold = threshold\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_idle = (y != REST_LABEL).astype(int)\n",
        "        self.model.fit(X, y_idle)\n",
        "        return self\n",
        "\n",
        "    def is_idle(self, window):\n",
        "        p_idle = self.model.predict_proba(window)[0, 0]\n",
        "\n",
        "        if p_idle < self.threshold:\n",
        "            return 0, p_idle\n",
        "        else:\n",
        "            return 1, p_idle\n",
        "\"\"\"\n",
        "\n",
        "class PseudoOnlineEvaluation():\n",
        "    \"\"\"\n",
        "    Evaluates windowed data in pseudo-online manner (simulating real time asynchronous BCI). It can be done within-session, evaluating only one session data, or inter-session, evaluating performance across sessions, without\n",
        "    breaking signal causality.\n",
        "\n",
        "    Parameters:\n",
        "        dataset: MOABB dataset\n",
        "            Dataset to be used. It has to be a MOABB dataset object.\n",
        "        pipelines: dict\n",
        "            Sklearn pipelines dictionary, as used in MOABB.\n",
        "        method: string\n",
        "            Either 'within-session' or 'inter-session'.\n",
        "                within-session: trains models on first k windows, testing on the remaining ones.\n",
        "                inter-session: trains models on first k sessions, testing on the remaining ones; doesn't violate data causality.\n",
        "        wsize: float\n",
        "            Window size.\n",
        "        wstep: int\n",
        "            Distance between consecutive windows (start to start).\n",
        "        subjects: list\n",
        "            List of subjects to process.\n",
        "        ratio: float\n",
        "            Proportion of data to be used in training.\n",
        "        threshold: float\n",
        "            Confidence threshold for classification.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, class_pipelines, method, wsize, wstep, subjects, idle_pipelines=None, two_stage=True, ratio=0.7, idle_threshold=0.6, task_threshold=0.6, no_run=False):\n",
        "        self.dataset = dataset\n",
        "        self.class_pipelines = class_pipelines\n",
        "        self.ratio = ratio\n",
        "        self.method = method\n",
        "        self.wsize = wsize\n",
        "        self.wstep = wstep\n",
        "        self.subjects = subjects\n",
        "        self.idle_threshold = idle_threshold\n",
        "        self.task_threshold = task_threshold\n",
        "        self.no_run = no_run\n",
        "        self.idle_pipelines = idle_pipelines\n",
        "        self.two_stage = two_stage\n",
        "\n",
        "        self.results_ = []\n",
        "        self.model_results_ = []\n",
        "\n",
        "        if self.two_stage is True and self.idle_pipelines is None:\n",
        "            raise AttributeError(\"Evaluation is set to two stage but has no idle_pipeline\")\n",
        "\n",
        "    def raw_concat(self, raw_list):\n",
        "        \"\"\"\n",
        "        Auxiliary function for raw data concat.\n",
        "        \"\"\"\n",
        "        if len(raw_list) == 0:\n",
        "            raise ValueError(\"Raw list is empty.\")\n",
        "        elif len(raw_list) == 1:    #aqui, o raw de uma sessao pode ser constituido de 1 ou mais runs, por isso essa verificação\n",
        "            if type(raw_list[0]) != list:\n",
        "                return raw_list[0]\n",
        "            else:\n",
        "                return mne.concatenate_raws(raw_list[0])\n",
        "        else:\n",
        "            return mne.concatenate_raws(raw_list)\n",
        "\n",
        "    def window_process(self, subject, sess, X_test, y_test, times_test):\n",
        "        # window processing loop\n",
        "        for window in range(len(X_test)):\n",
        "            window_start = times_test[window][0]\n",
        "            window_end = times_test[window][1]\n",
        "\n",
        "            pipe_window = np.array([X_test[window]])\n",
        "\n",
        "            \"\"\"\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            if (self.feature_pipeline != None):\n",
        "                feature_window = self.feature_pipeline.transform([X_test[window]])\n",
        "            else:\n",
        "                feature_window = np.array([X_test[window]])\n",
        "                idle_window = np.reshape(feature_window, (feature_window.shape[0], -1))\n",
        "\n",
        "            t_end = time.perf_counter()\n",
        "\n",
        "            t_transform = t_end - t_start\n",
        "            \"\"\"\n",
        "\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            if self.two_stage is True:\n",
        "                for idle_name, idle_pipe in self.idle_pipelines.items():\n",
        "                    idle_proba = idle_pipe.predict_proba(pipe_window)[0, 0]\n",
        "\n",
        "                    is_idle = 0 if idle_proba < self.idle_threshold else 1\n",
        "\n",
        "                    t_end = time.perf_counter()\n",
        "                    t_idle_detect = t_end - t_start\n",
        "\n",
        "                    task_proba = None\n",
        "\n",
        "                    if not is_idle:\n",
        "                        for name, pipe in self.class_pipelines.items():\n",
        "                            t_start = time.perf_counter()\n",
        "\n",
        "                            probs = pipe.predict_proba(pipe_window)[0]\n",
        "\n",
        "                            task_proba = np.max(probs)\n",
        "\n",
        "                            if task_proba < self.task_threshold:\n",
        "                                y_pred = REJECT_LABEL\n",
        "                            else:\n",
        "                                y_pred = probs.argmax()\n",
        "\n",
        "                            t_end = time.perf_counter()\n",
        "\n",
        "                            t_task_predict = t_end - t_start\n",
        "\n",
        "                            correct = (y_pred == y_test[window])\n",
        "\n",
        "                            res = {\n",
        "                                \"dataset\": self.dataset,\n",
        "                                \"subject\": subject,\n",
        "                                \"session\": sess,\n",
        "                                \"method\": self.method,\n",
        "                                \"idle_pipeline\": idle_name,\n",
        "                                \"task_pipeline\": name,\n",
        "                                \"window\": window,\n",
        "                                \"window_start\": window_start,\n",
        "                                \"window_end\": window_end,\n",
        "                                \"is_idle\": is_idle,\n",
        "                                \"t_idle_detect\": t_idle_detect,\n",
        "                                \"t_task_predict\": t_task_predict,\n",
        "                                \"t_predict\": (t_idle_detect + t_task_predict),\n",
        "                                \"y_pred\": y_pred,\n",
        "                                \"idle_proba\": idle_proba,\n",
        "                                \"task_proba\": task_proba,\n",
        "                                \"y_true\": y_test[window],\n",
        "                                \"correct\": correct\n",
        "                            }\n",
        "\n",
        "                            self.results_.append(res)\n",
        "                    else:\n",
        "                        y_pred = REST_LABEL\n",
        "                        t_task_predict = 0\n",
        "                        correct = (y_pred == y_test[window])\n",
        "\n",
        "                        res = {\n",
        "                            \"dataset\": self.dataset,\n",
        "                            \"subject\": subject,\n",
        "                            \"session\": sess,\n",
        "                            \"method\": self.method,\n",
        "                            \"idle_pipeline\": idle_name,\n",
        "                            \"task_pipeline\": None,\n",
        "                            \"window\": window,\n",
        "                            \"window_start\": window_start,\n",
        "                            \"window_end\": window_end,\n",
        "                            \"is_idle\": is_idle,\n",
        "                            \"t_idle_detect\": t_idle_detect,\n",
        "                            \"t_task_predict\": t_task_predict,\n",
        "                            \"t_predict\": (t_idle_detect + t_task_predict),\n",
        "                            \"y_pred\": y_pred,\n",
        "                            \"idle_proba\": idle_proba,\n",
        "                            \"task_proba\": task_proba,\n",
        "                            \"y_true\": y_test[window],\n",
        "                            \"correct\": correct\n",
        "                        }\n",
        "\n",
        "                        self.results_.append(res)\n",
        "            else:\n",
        "                for name, pipe in self.class_pipelines.items():\n",
        "                    t_start = time.perf_counter()\n",
        "\n",
        "                    probs = pipe.predict_proba(pipe_window)[0]\n",
        "\n",
        "                    task_proba = np.max(probs)\n",
        "                    idle_proba = probs[0]\n",
        "\n",
        "                    if task_proba < self.task_threshold:\n",
        "                        y_pred = REJECT_LABEL\n",
        "                    else:\n",
        "                        y_pred = probs.argmax()\n",
        "\n",
        "                    t_end = time.perf_counter()\n",
        "\n",
        "                    t_task_predict = t_end - t_start\n",
        "\n",
        "                    correct = (y_pred == y_test[window])\n",
        "\n",
        "                    res = {\n",
        "                        \"dataset\": self.dataset,\n",
        "                        \"subject\": subject,\n",
        "                        \"session\": sess,\n",
        "                        \"method\": self.method,\n",
        "                        \"idle_pipeline\": name,\n",
        "                        \"task_pipeline\": name,\n",
        "                        \"window\": window,\n",
        "                        \"window_start\": window_start,\n",
        "                        \"window_end\": window_end,\n",
        "                        \"is_idle\": int(y_pred == REST_LABEL),\n",
        "                        \"t_idle_detect\": t_task_predict if y_pred == REST_LABEL else None,\n",
        "                        \"t_task_predict\": t_task_predict,\n",
        "                        \"t_predict\": t_task_predict,\n",
        "                        \"y_pred\": y_pred,\n",
        "                        \"idle_proba\": idle_proba,\n",
        "                        \"task_proba\": task_proba,\n",
        "                        \"y_true\": y_test[window],\n",
        "                        \"correct\": correct\n",
        "                    }\n",
        "\n",
        "                    self.results_.append(res)\n",
        "\n",
        "    def idle_train(self, X_train, y_train):\n",
        "        y_idle = (y_train == REST_LABEL).astype(int)\n",
        "\n",
        "        for name, pipe in self.idle_pipelines.items():\n",
        "            print(f\"fitting idle pipeline {name}...\")\n",
        "\n",
        "            t_start = time.perf_counter()\n",
        "            pipe.fit(X_train, y_idle)\n",
        "            t_end = time.perf_counter()\n",
        "\n",
        "            t_idle_train = t_end - t_start\n",
        "\n",
        "            res = {\n",
        "                \"idle_pipeline\": name,\n",
        "                \"method\": self.method,\n",
        "                \"t_train\": t_idle_train\n",
        "\n",
        "            }\n",
        "\n",
        "            self.model_results_.append(res)\n",
        "\n",
        "            print(\"done fitting idle detector\")\n",
        "        return self\n",
        "\n",
        "    def task_train(self, X_train, y_train):\n",
        "        if self.two_stage is True:\n",
        "            # mask for task windows\n",
        "            mask = y_train != REST_LABEL\n",
        "            X_task = X_train[mask]\n",
        "            y_task = y_train[mask]\n",
        "        else:\n",
        "            X_task = X_train\n",
        "            y_task = y_train\n",
        "\n",
        "        # task classifier training\n",
        "        for name, pipe in self.class_pipelines.items():\n",
        "            print(f\"Fitting task pipeline {name}...\")\n",
        "            t_start = time.perf_counter()\n",
        "            pipe.fit(X_task, y_task)\n",
        "            t_end = time.perf_counter()\n",
        "\n",
        "            t_train = t_end - t_start\n",
        "            print(\"Done fitting!\")\n",
        "\n",
        "            res = {\n",
        "                \"task_pipeline\": name,\n",
        "                \"method\": self.method,\n",
        "                \"t_train\": t_train\n",
        "\n",
        "            }\n",
        "\n",
        "            self.model_results_.append(res)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Main function for processing data.\n",
        "        \"\"\"\n",
        "\n",
        "        for subject in self.subjects:\n",
        "            if subject not in self.dataset.subject_list:\n",
        "                raise ValueError(f\"Invalid subject index: {subject}\")\n",
        "            else:\n",
        "                print(f\"Processing subject {subject}...\")\n",
        "\n",
        "                raws_dict = {}\n",
        "                raws_test = {}\n",
        "                pre = self.dataset.get_data(subjects=[subject])\n",
        "\n",
        "                session_keys = []   # stores session ids (not always int)\n",
        "\n",
        "                if self.method == 'within-session':\n",
        "\n",
        "                    # raw extraction from moabb dataset\n",
        "                    for _, runs in pre.items():\n",
        "                        for sess, dicts in runs.items():\n",
        "                            session_keys.append(sess)\n",
        "                            raws_dict[sess] = []\n",
        "                            for _, data in dicts.items():\n",
        "                                raws_dict[sess].append(data)\n",
        "\n",
        "                    for sess in session_keys:\n",
        "                        print(f\"Processing session {sess} subject {subject}...\")\n",
        "                        raw = self.raw_concat(raws_dict[sess])\n",
        "                        events, event_ids = mne.events_from_annotations(raw)\n",
        "\n",
        "                        wgen = PseudoOnlineWindow(raw=raw,\n",
        "                                                events=events,\n",
        "                                                interval=self.dataset.interval,\n",
        "                                                task_ids=event_ids,\n",
        "                                                window_size=self.wsize,\n",
        "                                                window_step=self.wstep\n",
        "                                                )\n",
        "\n",
        "                        X, y, times = wgen.generate_windows()\n",
        "\n",
        "                        idx_split = int(len(X) * self.ratio)\n",
        "\n",
        "                        times_test = times[idx_split:]\n",
        "\n",
        "                        X_train, y_train = X[:idx_split], y[:idx_split]\n",
        "                        X_test, y_test = X[idx_split:], y[idx_split:]\n",
        "\n",
        "                        if (self.no_run):\n",
        "                            return X_train, y_train, X_test, y_test\n",
        "\n",
        "                        if self.two_stage is True:\n",
        "                            self.idle_train(X_train, y_train)\n",
        "\n",
        "                        self.task_train(X_train, y_train)\n",
        "\n",
        "                        self.window_process(subject, sess, X_test, y_test, times_test)\n",
        "\n",
        "\n",
        "                elif self.method == 'inter-session':\n",
        "                    if self.dataset.n_sessions > 1:\n",
        "                        session_split = int(self.ratio * self.dataset.n_sessions)\n",
        "                        raws_list = []\n",
        "                        raws_train = []\n",
        "\n",
        "                        print(f\"Splitting index is {session_split}, dataset has {self.dataset.n_sessions} sessions per subject.\")\n",
        "\n",
        "                        for _, runs in pre.items():\n",
        "                            for sess, dicts in runs.items():\n",
        "                                session_keys.append(sess)\n",
        "                                raws_test[sess] = []\n",
        "                                raws_dict[sess] = []\n",
        "                                for _, data in dicts.items():\n",
        "                                    raws_dict[sess].append(data)\n",
        "\n",
        "                        #essa verificação é porque eu acahva que o int() arredondava pra cima o valor... de todo jeito, nao faz mal deixar isso aqui\n",
        "                        if session_split == self.dataset.n_sessions:\n",
        "                            train_sessions = session_keys[:(session_split - 1)]\n",
        "                            test_sessions = session_keys[(session_split - 1):]\n",
        "                            for sess, data in raws_dict.items():\n",
        "                                if (session_keys.index(sess) + 1) < session_split:\n",
        "                                    raws_list.append(data)\n",
        "                                else:\n",
        "                                    raws_test[sess].append(data)\n",
        "                        else:\n",
        "                            train_sessions = session_keys[:session_split]\n",
        "                            test_sessions = session_keys[session_split:]\n",
        "                            for sess, data in raws_dict.items():\n",
        "                                if (session_keys.index(sess) + 1) <= session_split:\n",
        "                                    raws_list.append(data)\n",
        "                                else:\n",
        "                                    raws_test[sess].append(data)\n",
        "\n",
        "\n",
        "                        raws_train = self.raw_concat(raws_list)\n",
        "\n",
        "                        events, event_ids = mne.events_from_annotations(raws_train)\n",
        "\n",
        "                        wgen_train = PseudoOnlineWindow(raw=raws_train,\n",
        "                                                            events=events,\n",
        "                                                            interval=self.dataset.interval,\n",
        "                                                            task_ids=event_ids,\n",
        "                                                            window_size=self.wsize,\n",
        "                                                            window_step=self.wstep\n",
        "                                                            )\n",
        "\n",
        "                        X_train, y_train, times_train = wgen_train.generate_windows()\n",
        "\n",
        "                        if (self.no_run):\n",
        "                            return X_train, y_train, X_test, y_test\n",
        "\n",
        "                        if self.two_stage is True:\n",
        "                            self.idle_train(X_train, y_train)\n",
        "\n",
        "                        self.task_train(X_train, y_train)\n",
        "\n",
        "                        for sess in test_sessions:\n",
        "                            print(f\"Testing in session {sess}...\")\n",
        "\n",
        "                            raws = self.raw_concat(raws_test[sess])\n",
        "\n",
        "                            events, event_ids = mne.events_from_annotations(raws)\n",
        "\n",
        "                            wgen_test = PseudoOnlineWindow(raw=raws,\n",
        "                                                                events=events,\n",
        "                                                                interval=self.dataset.interval,\n",
        "                                                                task_ids=event_ids,\n",
        "                                                                window_size=self.wsize,\n",
        "                                                                window_step=self.wstep\n",
        "                                                                )\n",
        "\n",
        "                            X_test, y_test, times_test = wgen_test.generate_windows()\n",
        "\n",
        "                            self.window_process(subject, sess, X_test, y_test, times_test)\n",
        "                    else:\n",
        "                        raise ValueError(\"There are not enough sessions for evaluation.\")\n",
        "\n",
        "            if len(self.results_):\n",
        "                results_ = pd.DataFrame(self.results_)\n",
        "                results_.to_csv(f\"results-S{subject}.csv\", index=False)\n",
        "                self.results_ = []\n",
        "            if len(self.model_results_):\n",
        "                model_results_ = pd.DataFrame(self.model_results_)\n",
        "                model_results_.to_csv(f\"model-results-S{subject}.csv\", index=False)\n",
        "                self.model_results_ = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                   # mcc_acc_sess = matthews_corrcoef(y_test[:window+1], predictions_sess[:window+1])    #score acumulado dentro da sessão\n",
        "                                   # mcc_acc = matthews_corrcoef(y_all, predictions)   #score acumulado entre sessões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPmx2FnwDJdG",
        "outputId": "9dd15608-9c14-4306-9ee9-4a65a03eb204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 1...\n",
            "Processing session 0train subject 1...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "fitting idle pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.094 (2.2e-16 eps * 26 dim * 1.6e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=0 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "done fitting idle detector\n",
            "Fitting task pipeline csp+lda...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.067 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.067 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline riem+svc...\n",
            "Done fitting!\n",
            "Processing session 1test subject 1...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "fitting idle pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.094 (2.2e-16 eps * 26 dim * 1.6e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=0 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "done fitting idle detector\n",
            "Fitting task pipeline csp+lda...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.069 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.069 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline riem+svc...\n",
            "Done fitting!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat' to file '/content/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02T.mat'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'bnci-horizon-2020.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "100%|█████████████████████████████████████| 43.1M/43.1M [00:00<00:00, 38.1GB/s]\n",
            "SHA256 hash of downloaded file: 5ddd5cb520b1692c3ba1363f48d98f58f0e46f3699ee50d749947950fc39db27\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat' to file '/content/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02E.mat'.\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'bnci-horizon-2020.eu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "100%|█████████████████████████████████████| 44.2M/44.2M [00:00<00:00, 36.9GB/s]\n",
            "SHA256 hash of downloaded file: d63c454005d3a9b41d8440629482e855afc823339bdd0b5721842a7ee9cc7b12\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing session 0train subject 2...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "fitting idle pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.094 (2.2e-16 eps * 26 dim * 1.6e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=0 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "done fitting idle detector\n",
            "Fitting task pipeline csp+lda...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.069 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.069 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline riem+svc...\n",
            "Done fitting!\n",
            "Processing session 1test subject 2...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "fitting idle pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.094 (2.2e-16 eps * 26 dim * 1.6e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=0 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "done fitting idle detector\n",
            "Fitting task pipeline csp+lda...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.069 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline csp+svc...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 0.069 (2.2e-16 eps * 26 dim * 1.2e+13  max singular value)\n",
            "    Estimated rank (data): 26\n",
            "    data: rank 26 computed from 26 data channels with 0 projectors\n",
            "Reducing data rank from 26 -> 26\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "Done fitting!\n",
            "Fitting task pipeline riem+svc...\n",
            "Done fitting!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "dataset = BNCI2014_001()\n",
        "\n",
        "pipelines = {}\n",
        "idle_pipeline = {}\n",
        "\n",
        "idle_pipeline [\"csp+svc\"] = make_pipeline(ArrayFilter(sfreq=250, lfreq=8, hfreq=30), CSP(n_components=4), SVC(kernel='linear', probability=True))\n",
        "\n",
        "pipelines[\"csp+lda\"] = make_pipeline(ArrayFilter(sfreq=250, lfreq=8, hfreq=30), CSP(n_components=4), LDA())\n",
        "pipelines[\"csp+svc\"] = make_pipeline(ArrayFilter(sfreq=250, lfreq=8, hfreq=30), CSP(n_components=4), SVC(kernel='linear', probability=True))\n",
        "pipelines[\"riem+svc\"] = make_pipeline(ArrayFilter(sfreq=250, lfreq=8, hfreq=30), Covariances(estimator='oas'), TangentSpace(metric='logeuclid'), SVC(kernel='linear', probability=True))\n",
        "\n",
        "eval = PseudoOnlineEvaluation(dataset=dataset, two_stage=True, class_pipelines=pipelines, idle_pipelines=idle_pipeline, method='within-session', wsize=1, wstep=1, ratio=0.7, idle_threshold=0.55, task_threshold=0.55, subjects=[1, 2])\n",
        "eval.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50BIEspG9cu0"
      },
      "outputs": [],
      "source": [
        "def plot_inter_scores (df, subject, sess):\n",
        "  df_sub = df[(df[\"subject\"] == subject) & (df[\"session\"] == sess)].copy()\n",
        "\n",
        "  df_sub = df_sub.sort_values(\"window\")\n",
        "\n",
        "  plt.figure(figsize=(25, 10))\n",
        "  plt.plot(df_sub[\"window\"], df_sub[\"mcc_acc\"])\n",
        "  plt.xlabel(\"Window\")\n",
        "  plt.ylabel(\"Score acumulado\")\n",
        "  plt.title(f\"Sujeito {subject} - Sessão {sess}: Score Acumulado\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uRq77c8dOft"
      },
      "outputs": [],
      "source": [
        "plot_inter_scores(eval.results_, 1, '1test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvSf2+2RRpCSqU7OPMFuOP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}