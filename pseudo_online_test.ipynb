{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jooj-uni/neural-signal-analysis-bci/blob/master/pseudo_online_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C27fjABSCMjG",
        "outputId": "453dc8c1-f74b-4590-c27d-dd9c43ff7472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: mne in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2026.1.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: moabb in /usr/local/lib/python3.12/dist-packages (1.4.3)\n",
            "Collecting numpy>=2.0 (from moabb)\n",
            "  Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.16.3)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.11.0)\n",
            "Requirement already satisfied: pandas>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (3.15.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.13.2)\n",
            "Requirement already satisfied: pyriemann>=0.9 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.10)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (6.0.3)\n",
            "Requirement already satisfied: pooch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.8.2)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.32.4)\n",
            "Requirement already satisfied: urllib3>=1.26.15 in /usr/local/lib/python3.12/dist-packages (from moabb) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (4.67.1)\n",
            "Requirement already satisfied: coverage>=7.0.1 in /usr/local/lib/python3.12/dist-packages (from moabb) (7.13.1)\n",
            "Requirement already satisfied: memory-profiler>=0.61.0 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.61.0)\n",
            "Requirement already satisfied: edflib-python>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from moabb) (1.0.8)\n",
            "Requirement already satisfied: edfio>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.4.11)\n",
            "Requirement already satisfied: pytest>=8.3.5 in /usr/local/lib/python3.12/dist-packages (from moabb) (8.4.2)\n",
            "Requirement already satisfied: mne-bids>=0.16 in /usr/local/lib/python3.12/dist-packages (from moabb) (0.18.0)\n",
            "Collecting scikit-learn>=1.6 (from moabb)\n",
            "  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.2->moabb) (2.9.0.post0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory-profiler>=0.61.0->moabb) (5.9.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->moabb) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.2->moabb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.2->moabb) (2025.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.6.0->moabb) (4.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pyriemann>=0.9->moabb) (1.5.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.5->moabb) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28.1->moabb) (2026.1.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6->moabb) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.2->moabb) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->moabb) (3.0.3)\n",
            "Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.4.1 scikit-learn\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "681cffd6baa5418f9b9d652f7d623923"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting scikit-learn<1.4\n",
            "  Using cached scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2.0,>=1.17.3 (from scikit-learn<1.4)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.4) (3.6.0)\n",
            "Using cached scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy, scikit-learn\n",
            "  Attempting uninstall: numpy\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: numpy 2.4.1\n",
            "    Uninstalling numpy-2.4.1:\n",
            "      Successfully uninstalled numpy-2.4.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "moabb 1.4.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "moabb 1.4.3 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "esda 2.8.1 requires scikit-learn>=1.4, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mapclassify 2.10.0 requires scikit-learn>=1.4, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "spopt 0.7.0 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "libpysal 4.14.0 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "imbalanced-learn 0.14.1 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 scikit-learn\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "997acc1dece142cebfbe8ebc67506a16"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install mne\n",
        "!pip install moabb\n",
        "!pip install \"scikit-learn<1.4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j5u6USktCRM5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "import seaborn as sns\n",
        "from mne.decoding import CSP\n",
        "from pyriemann.classification import MDM\n",
        "from pyriemann.estimation import Covariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "\n",
        "import moabb\n",
        "from moabb.datasets import BNCI2014_001, Zhou2016, Stieger2021\n",
        "from moabb.evaluations import WithinSessionEvaluation\n",
        "from moabb.paradigms import LeftRightImagery\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7y3CGsryCY-z"
      },
      "outputs": [],
      "source": [
        "# Caminho seguro no Colab\n",
        "mne_path = \"/content/mne_data\"\n",
        "os.environ[\"MNE_DATA\"] = mne_path\n",
        "os.environ[\"MNE_DATASETS_SAMPLE_PATH\"] = mne_path\n",
        "os.environ[\"MOABB_RESULTS\"] = mne_path # Set MOABB results directory using os.environ\n",
        "os.makedirs(mne_path, exist_ok=True)\n",
        "\n",
        "# Forçar MNE a registrar o caminho internamente\n",
        "mne.set_config(\"MNE_DATA\", mne_path, set_env=True)\n",
        "mne.set_config(\"MNE_DATASETS_SAMPLE_PATH\", mne_path, set_env=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I7rylYNkZ7i3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import moabb\n",
        "import mne\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "REST_LABEL = 0\n",
        "REJECT_LABEL = -1\n",
        "\n",
        "class PseudoOnlineWindow():\n",
        "    \"\"\"\n",
        "    Segments data in windows for pseudo-online analysis.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "        raw: mne.Raw object\n",
        "            The continuous data.\n",
        "        events: arra\n",
        "            MNE event array.\n",
        "        interval: list\n",
        "            Dataset parameter defining imagery interval.\n",
        "        task_ids: dict\n",
        "            Defines the tasks and its numeric IDs. It can be used to select a subset of the dataset tasks.\n",
        "        window_size: float\n",
        "            The window size in seconds.\n",
        "        window_step: int\n",
        "            Distance in seconds between the start of two consecutive windows. It can be used to set superposition between windows, when value is lower than window_size.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, raw, events, interval, task_ids, window_size, window_step, chan_list=None):\n",
        "        self.raw = raw\n",
        "        self.events = events\n",
        "        self.interval = interval\n",
        "        self.sfreq = raw.info['sfreq']\n",
        "        self.task_ids = task_ids\n",
        "\n",
        "        self.window_size = int(window_size * self.sfreq)\n",
        "        self.window_step = int(window_step * self.sfreq)\n",
        "        self.chan_list = chan_list\n",
        "\n",
        "        self.t_start = int(interval[0] * self.sfreq)\n",
        "        self.t_end = int(interval[1] * self.sfreq)\n",
        "\n",
        "        self.labels = self.generate_labels()\n",
        "\n",
        "    def generate_labels(self):\n",
        "        \"\"\"\n",
        "        Attributes aa label for each sample. The label vector is initialized with 0 and each data point is attributed to the task label, if it is in imagery period.\n",
        "\n",
        "        Returns:\n",
        "            labels: nd array\n",
        "                Label vector containing labels for each data sample.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples = self.raw.n_times\n",
        "        labels = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "        valid_ids = list(self.task_ids.values())\n",
        "\n",
        "        for ev in self.events:\n",
        "            ev_idx, _, ev_id = ev\n",
        "\n",
        "            if ev_id in valid_ids:\n",
        "                # uses only imagery period for task attribution\n",
        "                start = ev_idx + self.t_start\n",
        "                stop = ev_idx + self.t_end\n",
        "\n",
        "                # ensure array limits\n",
        "                start = max(0, start)\n",
        "                stop = min(n_samples, stop)\n",
        "\n",
        "                labels[start:stop] = ev_id\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def generate_windows(self):\n",
        "        \"\"\"\n",
        "        Generates and labels windows.\n",
        "\n",
        "        Returns:\n",
        "            X: nd array shape=(n_windows, n_channels, n_times)\n",
        "                The windows (data).\n",
        "            y: nd array\n",
        "                Window labels in the same order as X.\n",
        "            times: nd array\n",
        "                Array of tuples. Each tuple is the timestamps (start and end) of each window. Might be useful for plotting.\n",
        "        \"\"\"\n",
        "        X, y, times = [], [], []\n",
        "\n",
        "        data = self.raw.get_data()\n",
        "        n_samples = data.shape[1]\n",
        "\n",
        "        for start_idx in range (0, n_samples - self.window_size, self.window_step):\n",
        "            end_idx = start_idx + self.window_size\n",
        "\n",
        "            if self.chan_list == None:\n",
        "                window_data = data[:, start_idx : end_idx]\n",
        "                window_labels = self.labels[start_idx:end_idx]\n",
        "            else:   #channel selection\n",
        "                window_data = []\n",
        "                for chan in self.chan_list:\n",
        "                    if chan in self.raw.ch_names:\n",
        "                        window_data.append(data[chan, start_idx : end_idx])\n",
        "                    else:\n",
        "                        raise ValueError(f\"Channel {chan} is not in {self.raw.ch_names}\")\n",
        "                window_labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "            count = np.bincount(window_labels)\n",
        "            major = np.argmax(count)\n",
        "\n",
        "            prop_major = count[major] / len(window_labels)\n",
        "\n",
        "            # class draw proportion\n",
        "            n_classes = len(np.unique(window_labels))\n",
        "            draw_prop = 1 / n_classes\n",
        "\n",
        "            # in case of draw, the posterior label wins\n",
        "            if prop_major != draw_prop:\n",
        "                y.append(major)\n",
        "            else:\n",
        "                y.append(window_labels[-1])\n",
        "\n",
        "            X.append(window_data)\n",
        "            times.append(((start_idx / self.sfreq), (end_idx / self.sfreq)))\n",
        "\n",
        "        return np.array(X), np.array(y), np.array(times)\n",
        "\n",
        "\n",
        "class IdleBaseline(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Applies baseline correction. It uses a fixed baseline. This transformer has to be applied to windowed data.\n",
        "\n",
        "    Parameters:\n",
        "        rest_label: int\n",
        "            Label representing idle state.\n",
        "\n",
        "    Returns:\n",
        "        X: nd array shape=(n_windows, n_channels, n_times)\n",
        "            Baseline corrected windowed data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rest_label=REST_LABEL):\n",
        "        self.rest_label = rest_label\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if y is None:\n",
        "            raise ValueError(\"Missing labels array\")\n",
        "\n",
        "        idle_windows = (y == self.rest_label)\n",
        "\n",
        "        if not np.any(idle_windows):\n",
        "            raise ValueError(\"There are no rest windows\")\n",
        "\n",
        "        self.baseline_ = X[idle_windows].mean(axis=0)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.subtract(X, self.baseline_)\n",
        "\n",
        "\n",
        "class PSD(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, sfreq, fmin=0, fmax=None):\n",
        "        self.sfreq = sfreq\n",
        "        self.fmin = fmin\n",
        "        self.fmax = fmax\n",
        "\n",
        "    def fit (self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        fmax = self.fmax if self.fmax is not None else self.sfreq / 2\n",
        "\n",
        "        psds, _ = mne.time_frequency.psd_array_welch(X,\n",
        "                                                     sfreq=self.sfreq,\n",
        "                                                     fmin=self.fmin,\n",
        "                                                     fmax=self.fmax,\n",
        "                                                     average='mean')\n",
        "\n",
        "        return psds\n",
        "\n",
        "class IdleDetection():\n",
        "    \"\"\"\n",
        "    Classifier for idle state detection.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, threshold=0.6):\n",
        "        self.threshold = threshold\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_idle = (y != REST_LABEL).astype(int)\n",
        "        self.model.fit(X, y_idle)\n",
        "        return self\n",
        "\n",
        "    def is_idle(self, window):\n",
        "        p_idle = self.model.predict_proba(window)[0, 0]\n",
        "\n",
        "        if p_idle < self.threshold:\n",
        "            return 0, p_idle\n",
        "        else:\n",
        "            return 1, p_idle\n",
        "\n",
        "\n",
        "class PseudoOnlineEvaluation():\n",
        "    \"\"\"\n",
        "    Evaluates windowed data in pseudo-online manner (simulating real time asynchronous BCI). It can be done within-session, evaluating only one session data, or inter-session, evaluating performance across sessions, without\n",
        "    breaking signal causality.\n",
        "\n",
        "    Feature pipeline should only be used to apply transformers common to all classification pipelines (such as IdleBaseline and PSD). The result in feature_pipeline has to preserve (n_samples, n_x, n_y) shape. If no feature_pipeline\n",
        "    is present, idle_detector will user reshaped data, and all classification pipelines will have to use a FunctionTransformer to reshape data, if only the classifier is used.\n",
        "\n",
        "    Parameters:\n",
        "        dataset: MOABB dataset\n",
        "            Dataset to be used. It has to be a MOABB dataset object.\n",
        "        pipelines: dict\n",
        "            Sklearn pipelines dictionary, as used in MOABB.\n",
        "        method: string\n",
        "            Either 'within-session' or 'inter-session'.\n",
        "                within-session: trains models on first k windows, testing on the remaining ones.\n",
        "                inter-session: trains models on first k sessions, testing on the remaining ones; doesn't violate data causality.\n",
        "        wsize: float\n",
        "            Window size.\n",
        "        wstep: int\n",
        "            Distance between consecutive windows (start to start).\n",
        "        subjects: list\n",
        "            List of subjects to process.\n",
        "        ratio: float\n",
        "            Proportion of data to be used in training.\n",
        "        threshold: float\n",
        "            Confidence threshold for classification.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, class_pipelines, method, wsize, wstep, subjects, idle_model, feature_pipeline=None, ratio=0.7, threshold=0.6, no_run=False):\n",
        "        self.dataset = dataset\n",
        "        self.feature_pipeline = feature_pipeline\n",
        "        self.class_pipelines = class_pipelines\n",
        "        self.ratio = ratio\n",
        "        self.method = method\n",
        "        self.wsize = wsize\n",
        "        self.wstep = wstep\n",
        "        self.subjects = subjects\n",
        "        self.task_threshold = threshold\n",
        "        self.no_run = no_run\n",
        "        self.idle_model = idle_model\n",
        "\n",
        "        self.results_ = []\n",
        "        self.model_results_ = []\n",
        "\n",
        "    def raw_concat(self, raw_list):\n",
        "        \"\"\"\n",
        "        Auxiliary function for raw data concat.\n",
        "        \"\"\"\n",
        "        if len(raw_list) == 0:\n",
        "            raise ValueError(\"Raw list is empty.\")\n",
        "        elif len(raw_list) == 1:    #aqui, o raw de uma sessao pode ser constituido de 1 ou mais runs, por isso essa verificação\n",
        "            if type(raw_list[0]) != list:\n",
        "                return raw_list[0]\n",
        "            else:\n",
        "                return mne.concatenate_raws(raw_list[0])\n",
        "        else:\n",
        "            return mne.concatenate_raws(raw_list)\n",
        "\n",
        "    def window_process(self, subject, sess, X_test, y_test, times_test):\n",
        "        # window processing loop\n",
        "        for window in range(len(X_test)):\n",
        "            window_start = times_test[window][0]\n",
        "            window_end = times_test[window][1]\n",
        "\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            if (self.feature_pipeline != None):\n",
        "                feature_window = self.feature_pipeline.transform([X_test[window]])\n",
        "            else:\n",
        "                feature_window = np.array([X_test[window]])\n",
        "                idle_window = np.reshape(feature_window, (feature_window.shape[0], -1))\n",
        "\n",
        "            t_end = time.perf_counter()\n",
        "\n",
        "            t_transform = t_end - t_start\n",
        "\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            idle_window, idle_proba = self.idle_detector.is_idle(idle_window)\n",
        "\n",
        "            t_end = time.perf_counter()\n",
        "            t_idle_detect = t_end - t_start\n",
        "\n",
        "            task_proba = None\n",
        "\n",
        "            if not idle_window:\n",
        "                for name, pipe in self.class_pipelines.items():\n",
        "                    t_start = time.perf_counter()\n",
        "\n",
        "                    probs = pipe.predict_proba(feature_window)[0]\n",
        "\n",
        "                    task_proba = np.max(probs)\n",
        "\n",
        "                    if task_proba < self.task_threshold:\n",
        "                        y_pred = REJECT_LABEL\n",
        "                    else:\n",
        "                        y_pred = probs.argmax()\n",
        "\n",
        "                    t_end = time.perf_counter()\n",
        "\n",
        "                    t_task_predict = t_end - t_start\n",
        "\n",
        "                    correct = (y_pred == y_test[window])\n",
        "\n",
        "                    res = {\n",
        "                        \"dataset\": self.dataset,\n",
        "                        \"subject\": subject,\n",
        "                        \"session\": sess,\n",
        "                        \"method\": self.method,\n",
        "                        \"pipeline\": name,\n",
        "                        \"window\": window,\n",
        "                        \"window_start\": window_start,\n",
        "                        \"window_end\": window_end,\n",
        "                        \"is_idle\": idle_window,\n",
        "                        \"t_transform\": t_transform,\n",
        "                        \"t_idle_detect\": t_idle_detect,\n",
        "                        \"t_task_predict\": t_task_predict,\n",
        "                        \"t_predict\": (t_transform + t_idle_detect + t_task_predict),\n",
        "                        \"y_pred\": y_pred,\n",
        "                        \"idle_proba\": idle_proba,\n",
        "                        \"task_proba\": task_proba,\n",
        "                        \"y_true\": y_test[window],\n",
        "                        \"correct\": correct\n",
        "                    }\n",
        "\n",
        "                    self.results_.append(res)\n",
        "            else:\n",
        "                y_pred = REST_LABEL\n",
        "                t_task_predict = 0\n",
        "                correct = (y_pred == y_test[window])\n",
        "\n",
        "                res = {\n",
        "                    \"dataset\": self.dataset,\n",
        "                    \"subject\": subject,\n",
        "                    \"session\": sess,\n",
        "                    \"method\": self.method,\n",
        "                    \"pipeline\": \"idle\",\n",
        "                    \"window\": window,\n",
        "                    \"window_start\": window_start,\n",
        "                    \"window_end\": window_end,\n",
        "                    \"is_idle\": idle_window,\n",
        "                    \"t_transform\": t_transform,\n",
        "                    \"t_idle_detect\": t_idle_detect,\n",
        "                    \"t_task_predict\": t_task_predict,\n",
        "                    \"t_predict\": (t_transform + t_idle_detect + t_task_predict),\n",
        "                    \"y_pred\": y_pred,\n",
        "                    \"idle_proba\": idle_proba,\n",
        "                    \"task_proba\": task_proba,\n",
        "                    \"y_true\": y_test[window],\n",
        "                    \"correct\": correct\n",
        "                }\n",
        "\n",
        "                self.results_.append(res)\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Main function for processing data.\n",
        "        \"\"\"\n",
        "\n",
        "        self.idle_detector = IdleDetection(self.idle_model)\n",
        "\n",
        "        for subject in self.subjects:\n",
        "            if subject not in self.dataset.subject_list:\n",
        "                raise ValueError(f\"Invalid subject index: {subject}\")\n",
        "            else:\n",
        "                print(f\"Processing subject {subject}...\")\n",
        "\n",
        "                raws_dict = {}\n",
        "                raws_test = {}\n",
        "                pre = self.dataset.get_data(subjects=[subject])\n",
        "\n",
        "                session_keys = []   # stores session ids (not always int)\n",
        "\n",
        "                if self.method == 'within-session':\n",
        "\n",
        "                    # raw extraction from moabb dataset\n",
        "                    for _, runs in pre.items():\n",
        "                        for sess, dicts in runs.items():\n",
        "                            session_keys.append(sess)\n",
        "                            raws_dict[sess] = []\n",
        "                            for _, data in dicts.items():\n",
        "                                raws_dict[sess].append(data)\n",
        "\n",
        "                    for sess in session_keys:\n",
        "                        print(f\"Processing session {sess} subject {subject}...\")\n",
        "                        raw = self.raw_concat(raws_dict[sess])\n",
        "                        events, event_ids = mne.events_from_annotations(raw)\n",
        "\n",
        "                        wgen = PseudoOnlineWindow(raw=raw,\n",
        "                                                events=events,\n",
        "                                                interval=self.dataset.interval,\n",
        "                                                task_ids=event_ids,\n",
        "                                                window_size=self.wsize,\n",
        "                                                window_step=self.wstep\n",
        "                                                )\n",
        "\n",
        "                        X, y, times = wgen.generate_windows()\n",
        "\n",
        "                        idx_split = int(len(X) * self.ratio)\n",
        "\n",
        "                        times_test = times[idx_split:]\n",
        "\n",
        "                        X_train, y_train = X[:idx_split], y[:idx_split]\n",
        "                        X_test, y_test = X[idx_split:], y[idx_split:]\n",
        "\n",
        "                        if (self.no_run):\n",
        "                            return X_train, y_train, X_test, y_test\n",
        "\n",
        "                        # feature pipeline training\n",
        "                        print(\"fitting feature pipeline...\")\n",
        "                        if (self.feature_pipeline != None):\n",
        "                          t_start = time.perf_counter()\n",
        "                          self.feature_pipeline.fit(X_train, y_train)\n",
        "                          t_end = time.perf_counter()\n",
        "\n",
        "                          t_feature_train = t_end - t_start\n",
        "\n",
        "                          res = {\n",
        "                              \"pipeline\": \"feature\",\n",
        "                              \"method\": self.method,\n",
        "                              \"t_train\": t_feature_train\n",
        "\n",
        "                          }\n",
        "\n",
        "                          self.model_results_.append(res)\n",
        "\n",
        "                          print(\"done fitting feature pipeline\")\n",
        "\n",
        "                          print(\"transforming training data...\")\n",
        "\n",
        "                          X_train = self.feature_pipeline.transform(X_train)\n",
        "\n",
        "                          print(\"done transforming training data\")\n",
        "\n",
        "                        # idle detector training\n",
        "                        print(\"fitting idle detector...\")\n",
        "                        if (self.feature_pipeline == None):\n",
        "                            idle_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "                        else:\n",
        "                            idle_train = X_train\n",
        "                        t_start = time.perf_counter()\n",
        "                        self.idle_detector.fit(idle_train, y_train)\n",
        "                        t_end = time.perf_counter()\n",
        "\n",
        "                        t_idle_train = t_end - t_start\n",
        "\n",
        "                        res = {\n",
        "                            \"pipeline\": \"idle\",\n",
        "                            \"method\": self.method,\n",
        "                            \"t_train\": t_idle_train\n",
        "\n",
        "                        }\n",
        "\n",
        "                        self.model_results_.append(res)\n",
        "\n",
        "                        print(\"done fitting idle detector\")\n",
        "\n",
        "                        # mask for task windows\n",
        "                        mask = y_train != REST_LABEL\n",
        "                        X_task = X_train[mask]\n",
        "                        y_task = y_train[mask]\n",
        "\n",
        "                        # task classifier training\n",
        "                        for name, model in self.class_pipelines.items():\n",
        "                            print(\"Fitting task classifier...\")\n",
        "                            t_start = time.perf_counter()\n",
        "                            model.fit(X_task, y_task)\n",
        "                            t_end = time.perf_counter()\n",
        "\n",
        "                            t_train = t_end - t_start\n",
        "                            print(\"Done fitting!\")\n",
        "\n",
        "                            res = {\n",
        "                                \"pipeline\": name,\n",
        "                                \"method\": self.method,\n",
        "                                \"t_train\": t_train\n",
        "\n",
        "                            }\n",
        "\n",
        "                            self.model_results_.append(res)\n",
        "\n",
        "                        self.window_process(subject, sess, X_test, y_test, times_test)\n",
        "\n",
        "\n",
        "                elif self.method == 'inter-session':\n",
        "                    if self.dataset.n_sessions > 1:\n",
        "                        session_split = int(self.ratio * self.dataset.n_sessions)\n",
        "                        raws_list = []\n",
        "                        raws_train = []\n",
        "\n",
        "                        print(f\"Splitting index is {session_split}, dataset has {self.dataset.n_sessions} sessions per subject.\")\n",
        "\n",
        "                        for _, runs in pre.items():\n",
        "                            for sess, dicts in runs.items():\n",
        "                                session_keys.append(sess)\n",
        "                                raws_test[sess] = []\n",
        "                                raws_dict[sess] = []\n",
        "                                for _, data in dicts.items():\n",
        "                                    raws_dict[sess].append(data)\n",
        "\n",
        "                        #essa verificação é porque eu acahva que o int() arredondava pra cima o valor... de todo jeito, nao faz mal deixar isso aqui\n",
        "                        if session_split == self.dataset.n_sessions:\n",
        "                            train_sessions = session_keys[:(session_split - 1)]\n",
        "                            test_sessions = session_keys[(session_split - 1):]\n",
        "                            for sess, data in raws_dict.items():\n",
        "                                if (session_keys.index(sess) + 1) < session_split:\n",
        "                                    raws_list.append(data)\n",
        "                                else:\n",
        "                                    raws_test[sess].append(data)\n",
        "                        else:\n",
        "                            train_sessions = session_keys[:session_split]\n",
        "                            test_sessions = session_keys[session_split:]\n",
        "                            for sess, data in raws_dict.items():\n",
        "                                if (session_keys.index(sess) + 1) <= session_split:\n",
        "                                    raws_list.append(data)\n",
        "                                else:\n",
        "                                    raws_test[sess].append(data)\n",
        "\n",
        "\n",
        "                        raws_train = self.raw_concat(raws_list)\n",
        "\n",
        "                        events, event_ids = mne.events_from_annotations(raws_train)\n",
        "\n",
        "                        wgen_train = PseudoOnlineWindow(raw=raws_train,\n",
        "                                                            events=events,\n",
        "                                                            interval=self.dataset.interval,\n",
        "                                                            task_ids=event_ids,\n",
        "                                                            window_size=self.wsize,\n",
        "                                                            window_step=self.wstep\n",
        "                                                            )\n",
        "\n",
        "                        X_train, y_train, times_train = wgen_train.generate_windows()\n",
        "\n",
        "                        if (self.no_run):\n",
        "                            return X_train, y_train, X_test, y_test\n",
        "\n",
        "                        # feature pipeline training\n",
        "                        print(\"fitting feature pipeline...\")\n",
        "                        t_start = time.perf_counter()\n",
        "                        self.feature_pipeline.fit(X_train, y_train)\n",
        "                        t_end = time.perf_counter()\n",
        "\n",
        "                        t_feature_train = t_end - t_start\n",
        "\n",
        "                        res = {\n",
        "                            \"pipeline\": \"feature\",\n",
        "                            \"method\": self.method,\n",
        "                            \"t_train\": t_feature_train\n",
        "\n",
        "                        }\n",
        "\n",
        "                        self.model_results_.append(res)\n",
        "\n",
        "                        print(\"done fitting feature pipeline\")\n",
        "\n",
        "                        print(\"transforming training data...\")\n",
        "\n",
        "                        X_train = self.feature_pipeline.transform(X_train)\n",
        "\n",
        "                        print(\"done transforming training data\")\n",
        "\n",
        "                        # idle detector training\n",
        "                        print(\"fittting idle detector...\")\n",
        "                        t_start = time.perf_counter()\n",
        "                        self.idle_detector.fit(X_train, y_train)\n",
        "                        t_end = time.perf_counter()\n",
        "\n",
        "                        t_idle_train = t_end - t_start\n",
        "\n",
        "                        res = {\n",
        "                            \"pipeline\": \"idle\",\n",
        "                            \"method\": self.method,\n",
        "                            \"t_train\": t_idle_train\n",
        "\n",
        "                        }\n",
        "\n",
        "                        self.model_results_.append(res)\n",
        "\n",
        "                        print(\"done fitting idle detector\")\n",
        "\n",
        "                        # mask for task windows\n",
        "                        mask = y_train != REST_LABEL\n",
        "                        X_task = X_train[mask]\n",
        "                        y_task = y_train[mask]\n",
        "\n",
        "                        # task classifier training\n",
        "                        for name, model in self.class_pipelines.items():\n",
        "                            print(f\"Fitting task classifier {name}...\")\n",
        "                            t_start = time.perf_counter()\n",
        "                            model.fit(X_task, y_task)\n",
        "                            t_end = time.perf_counter()\n",
        "\n",
        "                            t_train = t_end - t_start\n",
        "                            print(\"Done fitting!\")\n",
        "\n",
        "                            res = {\n",
        "                                \"pipeline\": name,\n",
        "                                \"method\": self.method,\n",
        "                                \"t_train\": t_train\n",
        "\n",
        "                            }\n",
        "\n",
        "                            self.model_results_.append(res)\n",
        "\n",
        "                        for sess in test_sessions:\n",
        "                            print(f\"Testing in session {sess}...\")\n",
        "\n",
        "                            raws = self.raw_concat(raws_test[sess])\n",
        "\n",
        "                            events, event_ids = mne.events_from_annotations(raws)\n",
        "\n",
        "                            wgen_test = PseudoOnlineWindow(raw=raws,\n",
        "                                                                events=events,\n",
        "                                                                interval=self.dataset.interval,\n",
        "                                                                task_ids=event_ids,\n",
        "                                                                window_size=self.wsize,\n",
        "                                                                window_step=self.wstep\n",
        "                                                                )\n",
        "\n",
        "                            X_test, y_test, times_test = wgen_test.generate_windows()\n",
        "\n",
        "                            self.window_process(subject, sess, X_test, y_test, times_test)\n",
        "                    else:\n",
        "                        raise ValueError(\"There are not enough sessions for evaluation.\")\n",
        "\n",
        "            if len(self.results_):\n",
        "                results_ = pd.DataFrame(self.results_)\n",
        "                results_.to_csv(f\"results-S{subject}.csv\", index=False)\n",
        "                self.results_ = []\n",
        "            if len(self.model_results_):\n",
        "                model_results_ = pd.DataFrame(self.model_results_)\n",
        "                model_results_.to_csv(f\"model-results-S{subject}.csv\", index=False)\n",
        "                self.model_results_ = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                   # mcc_acc_sess = matthews_corrcoef(y_test[:window+1], predictions_sess[:window+1])    #score acumulado dentro da sessão\n",
        "                                   # mcc_acc = matthews_corrcoef(y_all, predictions)   #score acumulado entre sessões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "FPmx2FnwDJdG",
        "outputId": "5c8c81c7-218e-4839-cde5-ffba922caefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subject 1...\n",
            "Processing session 0train subject 1...\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "fitting feature pipeline...\n",
            "done fitting feature pipeline\n",
            "transforming training data...\n",
            "done transforming training data\n",
            "fitting idle detector...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done fitting idle detector\n",
            "Fitting task classifier...\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 4.7e+02 (2.2e-16 eps * 2011 dim * 1.1e+15  max singular value)\n",
            "    Estimated rank (data): 2011\n",
            "    data: rank 2011 computed from 2600 data channels with 0 projectors\n",
            "    Setting small data eigenvalues to zero (without PCA)\n",
            "Reducing data rank from 2600 -> 2011\n",
            "Estimating class=1 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=2 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=3 covariance using EMPIRICAL\n",
            "Done.\n",
            "Estimating class=4 covariance using EMPIRICAL\n",
            "Done.\n",
            "    Setting small data eigenvalues to zero (without PCA)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1775988611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0meval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPseudoOnlineEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midle_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midle_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_pipelines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'within-session'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3401452372.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting task classifier...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                             \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                             \u001b[0mt_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[1;32m    422\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/decoding/csp.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# use parent TransformerMixin method but with custom docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"get_spatial_filter_from_estimator(clf, info=info).plot_patterns()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/decoding/csp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Covariance estimation, GED/AJD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# and evecs/evals sorting happen here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mpick_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/decoding/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mevecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_smart_ajd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestr_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/decoding/_ged.py\u001b[0m in \u001b[0;36m_smart_ajd\u001b[0;34m(covs, restr_mat, weights)\u001b[0m\n\u001b[1;32m    119\u001b[0m             )\n\u001b[1;32m    120\u001b[0m         \u001b[0mcovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrestr_mat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mrestr_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcovs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mevecs_restr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ajd_pham\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mevecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_eigenvectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevecs_restr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mevecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestr_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mevecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mne/decoding/csp.py\u001b[0m in \u001b[0;36m_ajd_pham\u001b[0;34m(X, eps, max_iter)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_times\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;31m# handle matrix builder syntax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "dataset = BNCI2014_001()\n",
        "\n",
        "idle_model = LogisticRegression()\n",
        "\n",
        "pipelines = {}\n",
        "pipelines[\"csp+lda\"] = make_pipeline(CSP(n_components=4), LDA())\n",
        "\n",
        "feature_pipeline = Pipeline([('reshape', FunctionTransformer(lambda x: x.reshape((x.shape[0], -1)))),('scaler', StandardScaler())])\n",
        "\n",
        "eval = PseudoOnlineEvaluation(dataset=dataset, idle_model=idle_model, feature_pipeline=feature_pipeline, class_pipelines=pipelines, method='within-session', wsize=0.4, wstep=0.4, ratio=0.7, threshold=0.55, subjects=[1, 2])\n",
        "eval.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50BIEspG9cu0"
      },
      "outputs": [],
      "source": [
        "def plot_inter_scores (df, subject, sess):\n",
        "  df_sub = df[(df[\"subject\"] == subject) & (df[\"session\"] == sess)].copy()\n",
        "\n",
        "  df_sub = df_sub.sort_values(\"window\")\n",
        "\n",
        "  plt.figure(figsize=(25, 10))\n",
        "  plt.plot(df_sub[\"window\"], df_sub[\"mcc_acc\"])\n",
        "  plt.xlabel(\"Window\")\n",
        "  plt.ylabel(\"Score acumulado\")\n",
        "  plt.title(f\"Sujeito {subject} - Sessão {sess}: Score Acumulado\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uRq77c8dOft"
      },
      "outputs": [],
      "source": [
        "plot_inter_scores(eval.results_, 1, '1test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3l/1ILwvXmRh112kaIhJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}